{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\박성제\\AppData\\Local\\Temp\\ipykernel_24360\\3111893703.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: -96.79945066352593, Epsilon: 0.99\n",
      "Episode: 1, Reward: -103.27350002208057, Epsilon: 0.99\n",
      "Episode: 2, Reward: -262.36922462623875, Epsilon: 0.99\n",
      "Episode: 3, Reward: -310.22056720728284, Epsilon: 0.98\n",
      "Episode: 4, Reward: -209.79776022013164, Epsilon: 0.98\n",
      "Episode: 5, Reward: -347.3763375745901, Epsilon: 0.97\n",
      "Episode: 6, Reward: -281.9818341131228, Epsilon: 0.97\n",
      "Episode: 7, Reward: -77.18537028606973, Epsilon: 0.96\n",
      "Episode: 8, Reward: -121.83518405869063, Epsilon: 0.96\n",
      "Episode: 9, Reward: -330.763485156463, Epsilon: 0.95\n",
      "Episode: 10, Reward: -117.05078128570642, Epsilon: 0.95\n",
      "Episode: 11, Reward: -332.9607124380012, Epsilon: 0.94\n",
      "Episode: 12, Reward: -209.6830852408756, Epsilon: 0.94\n",
      "Episode: 13, Reward: -121.91174430071067, Epsilon: 0.93\n",
      "Episode: 14, Reward: -27.239083293772907, Epsilon: 0.93\n",
      "Episode: 15, Reward: -102.54991448781844, Epsilon: 0.92\n",
      "Episode: 16, Reward: -160.00554089320786, Epsilon: 0.92\n",
      "Episode: 17, Reward: -242.15690045323478, Epsilon: 0.91\n",
      "Episode: 18, Reward: -81.43595848452722, Epsilon: 0.91\n",
      "Episode: 19, Reward: -114.80892537918893, Epsilon: 0.90\n",
      "Episode: 20, Reward: -160.6333568455548, Epsilon: 0.90\n",
      "Episode: 21, Reward: -50.6606872853812, Epsilon: 0.90\n",
      "Episode: 22, Reward: -80.72074697807481, Epsilon: 0.89\n",
      "Episode: 23, Reward: -277.33427976233986, Epsilon: 0.89\n",
      "Episode: 24, Reward: -62.9465843930621, Epsilon: 0.88\n",
      "Episode: 25, Reward: -82.86933189314053, Epsilon: 0.88\n",
      "Episode: 26, Reward: -96.48622419463435, Epsilon: 0.87\n",
      "Episode: 27, Reward: -174.91374625023911, Epsilon: 0.87\n",
      "Episode: 28, Reward: -85.59186711990294, Epsilon: 0.86\n",
      "Episode: 29, Reward: -289.5501076908132, Epsilon: 0.86\n",
      "Episode: 30, Reward: -81.80248751410377, Epsilon: 0.86\n",
      "Episode: 31, Reward: -35.623202338511376, Epsilon: 0.85\n",
      "Episode: 32, Reward: -237.91852930786342, Epsilon: 0.85\n",
      "Episode: 33, Reward: -112.18887014999149, Epsilon: 0.84\n",
      "Episode: 34, Reward: -161.93146221928174, Epsilon: 0.84\n",
      "Episode: 35, Reward: -233.2574639901731, Epsilon: 0.83\n",
      "Episode: 36, Reward: -79.67972158258326, Epsilon: 0.83\n",
      "Episode: 37, Reward: -220.04602960285496, Epsilon: 0.83\n",
      "Episode: 38, Reward: -65.94785396269027, Epsilon: 0.82\n",
      "Episode: 39, Reward: -49.226383154503424, Epsilon: 0.82\n",
      "Episode: 40, Reward: -159.13005554001182, Epsilon: 0.81\n",
      "Episode: 41, Reward: -245.20025214348718, Epsilon: 0.81\n",
      "Episode: 42, Reward: -293.5918158950378, Epsilon: 0.81\n",
      "Episode: 43, Reward: -220.67021217150295, Epsilon: 0.80\n",
      "Episode: 44, Reward: -75.18242510071869, Epsilon: 0.80\n",
      "Episode: 45, Reward: -245.08328549385527, Epsilon: 0.79\n",
      "Episode: 46, Reward: -138.60974999058664, Epsilon: 0.79\n",
      "Episode: 47, Reward: -24.18091931853604, Epsilon: 0.79\n",
      "Episode: 48, Reward: -32.52307221476654, Epsilon: 0.78\n",
      "Episode: 49, Reward: -141.95404221617213, Epsilon: 0.78\n",
      "Episode: 50, Reward: -60.28226241926727, Epsilon: 0.77\n",
      "Episode: 51, Reward: -92.614429142141, Epsilon: 0.77\n",
      "Episode: 52, Reward: -153.38055698751242, Epsilon: 0.77\n",
      "Episode: 53, Reward: -67.39092768506617, Epsilon: 0.76\n",
      "Episode: 54, Reward: -23.56444074610593, Epsilon: 0.76\n",
      "Episode: 55, Reward: -174.09159021914272, Epsilon: 0.76\n",
      "Episode: 56, Reward: -88.0930481041423, Epsilon: 0.75\n",
      "Episode: 57, Reward: -2.4445486274574364, Epsilon: 0.75\n",
      "Episode: 58, Reward: 55.537618855021435, Epsilon: 0.74\n",
      "Episode: 59, Reward: -102.96329406123486, Epsilon: 0.74\n",
      "Episode: 60, Reward: -71.12502867189846, Epsilon: 0.74\n",
      "Episode: 61, Reward: -65.0552564805451, Epsilon: 0.73\n",
      "Episode: 62, Reward: -33.09026849831062, Epsilon: 0.73\n",
      "Episode: 63, Reward: -116.99485384842822, Epsilon: 0.73\n",
      "Episode: 64, Reward: -88.74582194010978, Epsilon: 0.72\n",
      "Episode: 65, Reward: -100.43994431055107, Epsilon: 0.72\n",
      "Episode: 66, Reward: -77.60369749457095, Epsilon: 0.71\n",
      "Episode: 67, Reward: -235.93251920459025, Epsilon: 0.71\n",
      "Episode: 68, Reward: -16.113989327184655, Epsilon: 0.71\n",
      "Episode: 69, Reward: 13.867325888883386, Epsilon: 0.70\n",
      "Episode: 70, Reward: -0.3750720495673221, Epsilon: 0.70\n",
      "Episode: 71, Reward: -117.21095769329563, Epsilon: 0.70\n",
      "Episode: 72, Reward: -105.55312097847192, Epsilon: 0.69\n",
      "Episode: 73, Reward: -114.97679358905552, Epsilon: 0.69\n",
      "Episode: 74, Reward: -343.9467448317441, Epsilon: 0.69\n",
      "Episode: 75, Reward: -40.078761064425905, Epsilon: 0.68\n",
      "Episode: 76, Reward: -62.554316522659754, Epsilon: 0.68\n",
      "Episode: 77, Reward: 9.811594230858404, Epsilon: 0.68\n",
      "Episode: 78, Reward: -97.39610614253998, Epsilon: 0.67\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "EPISODES = 1000\n",
    "GAMMA = 0.99\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 16\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "\n",
    "# Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_state, n_action):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_state, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_action)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize networks and replay buffer\n",
    "policy_net = QNetwork(n_state, n_action)\n",
    "target_net = QNetwork(n_state, n_action)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# Training loop\n",
    "epsilon = EPSILON_START\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = policy_net(torch.FloatTensor(state)).detach().numpy()\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update policy\n",
    "        if len(replay_buffer) > BATCH_SIZE:\n",
    "            batch = replay_buffer.sample(BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = torch.FloatTensor(states)\n",
    "            actions = torch.LongTensor(actions)\n",
    "            rewards = torch.FloatTensor(rewards)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            dones = torch.FloatTensor(dones)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "            next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "            targets = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "\n",
    "            loss = nn.MSELoss()(q_values, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Update target network\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Epsilon decay\n",
    "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "    print(f\"Episode: {episode}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
